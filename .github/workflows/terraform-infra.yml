
name: Terraform Infrastructure

on:
  push:
    branches: [ main ]
    paths: [ 'terraform/' ]
  pull_request:
    branches: [ main ]
    paths: [ 'terraform/' ]
  workflow_dispatch:

env:
  AWS_REGION: us-east-1
  TF_VERSION: 1.5.0
  S3_BUCKET: retail-store-terraform-state

jobs:
  terraform-plan:
    name: Terraform Plan
    runs-on: ubuntu-latest
    if: github.event_name == 'pull_request'

    steps:
    - name: Checkout
      uses: actions/checkout@v4

    - name: Setup Terraform
      uses: hashicorp/setup-terraform@v3
      with:
        terraform_version: ${{ env.TF_VERSION }}

    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ env.AWS_REGION }}

    - name: Terraform Init
      working-directory: terraform/eks/minimal
      run: |
        terraform init -backend-config="bucket=${{ env.S3_BUCKET }}" \
                        -backend-config="key=terraform.tfstate" \
                        -backend-config="region=${{ env.AWS_REGION }}" \
                        -backend-config="dynamodb_table=retail-store-terraform-locks" \
                        -backend-config="encrypt=true"

    - name: Terraform Plan
      working-directory: terraform/eks/minimal
      run: terraform plan -no-color
      continue-on-error: true

  terraform-apply:
    name: Terraform Apply
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main' && (github.event_name == 'push' || github.event_name == 'workflow_dispatch')

    steps:
    - name: Checkout
      uses: actions/checkout@v4

    - name: Setup Terraform
      uses: hashicorp/setup-terraform@v3
      with:
        terraform_version: ${{ env.TF_VERSION }}

    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ env.AWS_REGION }}

    - name: Create S3 bucket
      run: |
        aws s3api create-bucket --bucket ${{ env.S3_BUCKET }} --region ${{ env.AWS_REGION }} || echo "Bucket already exists"
        aws s3api put-bucket-versioning --bucket ${{ env.S3_BUCKET }} --versioning-configuration Status=Enabled

    - name: Terraform Init
      working-directory: terraform/eks/minimal
      run: |
        terraform init -backend-config="bucket=${{ env.S3_BUCKET }}" \
                        -backend-config="key=terraform.tfstate" \
                        -backend-config="region=${{ env.AWS_REGION }}" \
                        -backend-config="dynamodb_table=retail-store-terraform-locks" \
                        -backend-config="encrypt=true"

    - name: Handle Existing Resources
      working-directory: terraform/eks/minimal
      run: |
        echo "Importing existing resources..."
        
        # Check if resources exist in state, if not try to import
        if ! terraform state show 'module.retail_app_eks.module.eks_cluster.module.kms.aws_kms_alias.this["cluster"]' >/dev/null 2>&1; then
          terraform import 'module.retail_app_eks.module.eks_cluster.module.kms.aws_kms_alias.this["cluster"]' alias/eks/retail-store 2>/dev/null || echo "KMS alias import failed or not found"
        fi
        
        if ! terraform state show 'module.retail_app_eks.module.eks_cluster.aws_cloudwatch_log_group.this[0]' >/dev/null 2>&1; then
          terraform import 'module.retail_app_eks.module.eks_cluster.aws_cloudwatch_log_group.this[0]' /aws/eks/retail-store/cluster 2>/dev/null || echo "Log group import failed or not found"
        fi
        
        echo "Import completed"

    - name: Clean up AWS Load Balancer Controller
      run: |
        aws eks update-kubeconfig --region ${{ env.AWS_REGION }} --name retail-store || echo "Cluster not found"
        kubectl delete validatingwebhookconfiguration aws-load-balancer-webhook --ignore-not-found=true
        kubectl delete mutatingwebhookconfiguration aws-load-balancer-webhook --ignore-not-found=true
        kubectl delete deployment aws-load-balancer-controller -n kube-system --ignore-not-found=true
        kubectl delete service aws-load-balancer-webhook-service -n kube-system --ignore-not-found=true

    - name: Terraform Apply
      working-directory: terraform/eks/minimal
      run: terraform apply -auto-approve

    - name: Output cluster info
      working-directory: terraform/eks/minimal
      run: |
        echo "EKS Cluster deployed successfully!"
        terraform output configure_kubectl