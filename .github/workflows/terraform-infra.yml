
name: Terraform Infrastructure

on:
  push:
    branches: [ main ]
    paths: [ 'terraform/' ]
  pull_request:
    branches: [ main ]
    paths: [ 'terraform/' ]
  workflow_dispatch:

env:
  AWS_REGION: us-east-1
  TF_VERSION: 1.5.0
  S3_BUCKET: retail-store-terraform-state

jobs:
  terraform-plan:
    name: Terraform Plan
    runs-on: ubuntu-latest
    if: github.event_name == 'pull_request'

    steps:
    - name: Checkout
      uses: actions/checkout@v4

    - name: Setup Terraform
      uses: hashicorp/setup-terraform@v3
      with:
        terraform_version: ${{ env.TF_VERSION }}

    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ env.AWS_REGION }}

    - name: Terraform Init
      working-directory: terraform/eks/minimal
      run: |
        terraform init -backend-config="bucket=${{ env.S3_BUCKET }}" \
                        -backend-config="key=terraform.tfstate" \
                        -backend-config="region=${{ env.AWS_REGION }}" \
                        -backend-config="dynamodb_table=retail-store-terraform-locks" \
                        -backend-config="encrypt=true"

    - name: Terraform Plan
      working-directory: terraform/eks/minimal
      run: terraform plan -no-color
      continue-on-error: true

  terraform-apply:
    name: Terraform Apply
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main' && (github.event_name == 'push' || github.event_name == 'workflow_dispatch')

    steps:
    - name: Checkout
      uses: actions/checkout@v4

    - name: Setup Terraform
      uses: hashicorp/setup-terraform@v3
      with:
        terraform_version: ${{ env.TF_VERSION }}

    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ env.AWS_REGION }}

    - name: Create S3 bucket
      run: |
        aws s3api create-bucket --bucket ${{ env.S3_BUCKET }} --region ${{ env.AWS_REGION }} || echo "Bucket already exists"
        aws s3api put-bucket-versioning --bucket ${{ env.S3_BUCKET }} --versioning-configuration Status=Enabled

    - name: Terraform Init
      working-directory: terraform/eks/minimal
      run: |
        terraform init -backend-config="bucket=${{ env.S3_BUCKET }}" \
                        -backend-config="key=terraform.tfstate" \
                        -backend-config="region=${{ env.AWS_REGION }}" \
                        -backend-config="dynamodb_table=retail-store-terraform-locks" \
                        -backend-config="encrypt=true"

    - name: Clean up existing resources
      run: |
        echo "Cleaning up existing resources..."
        
        # Delete existing KMS alias if it exists
        aws kms delete-alias --alias-name alias/eks/retail-store 2>/dev/null || echo "KMS alias not found or already deleted"
        
        # Delete existing log group
        aws logs delete-log-group --log-group-name /aws/eks/retail-store/cluster 2>/dev/null || echo "Log group not found or already deleted"
        
        # Clean up VPCs with dependencies
        for vpc_id in $(aws ec2 describe-vpcs --query 'Vpcs[?IsDefault==`false`].VpcId' --output text); do
          vpc_name=$(aws ec2 describe-vpcs --vpc-ids $vpc_id --query 'Vpcs[0].Tags[?Key==`Name`].Value' --output text 2>/dev/null || echo "")
          if [ "$vpc_name" != "retail-store-vpc" ] && [ -n "$vpc_id" ]; then
            echo "Cleaning up VPC: $vpc_id ($vpc_name)"
            
            # Delete NAT gateways
            aws ec2 describe-nat-gateways --filter "Name=vpc-id,Values=$vpc_id" --query 'NatGateways[].NatGatewayId' --output text | xargs -r -n1 aws ec2 delete-nat-gateway --nat-gateway-id 2>/dev/null || true
            
            # Delete internet gateways
            aws ec2 describe-internet-gateways --filters "Name=attachment.vpc-id,Values=$vpc_id" --query 'InternetGateways[].InternetGatewayId' --output text | xargs -r -n1 -I {} sh -c 'aws ec2 detach-internet-gateway --internet-gateway-id {} --vpc-id '$vpc_id' && aws ec2 delete-internet-gateway --internet-gateway-id {}' 2>/dev/null || true
            
            # Delete subnets
            aws ec2 describe-subnets --filters "Name=vpc-id,Values=$vpc_id" --query 'Subnets[].SubnetId' --output text | xargs -r -n1 aws ec2 delete-subnet --subnet-id 2>/dev/null || true
            
            # Delete route tables (except main)
            aws ec2 describe-route-tables --filters "Name=vpc-id,Values=$vpc_id" --query 'RouteTables[?Associations[0].Main!=`true`].RouteTableId' --output text | xargs -r -n1 aws ec2 delete-route-table --route-table-id 2>/dev/null || true
            
            # Delete security groups (except default)
            aws ec2 describe-security-groups --filters "Name=vpc-id,Values=$vpc_id" --query 'SecurityGroups[?GroupName!=`default`].GroupId' --output text | xargs -r -n1 aws ec2 delete-security-group --group-id 2>/dev/null || true
            
            # Finally delete VPC
            sleep 10
            aws ec2 delete-vpc --vpc-id $vpc_id 2>/dev/null || echo "Failed to delete VPC $vpc_id"
          fi
        done
        
        echo "Cleanup completed"

    - name: Handle Existing Resources
      working-directory: terraform/eks/minimal
      run: |
        echo "Checking terraform state..."
        terraform state list || echo "No existing state"
        echo "State check completed"

    - name: Clean up AWS Load Balancer Controller
      run: |
        aws eks update-kubeconfig --region ${{ env.AWS_REGION }} --name retail-store || echo "Cluster not found"
        
        # Delete all webhook configurations
        kubectl get validatingwebhookconfiguration -o name | grep -E '(aws-load-balancer|elbv2)' | xargs -r kubectl delete --ignore-not-found=true
        kubectl get mutatingwebhookconfiguration -o name | grep -E '(aws-load-balancer|elbv2)' | xargs -r kubectl delete --ignore-not-found=true
        
        # Force delete specific webhooks
        kubectl patch validatingwebhookconfiguration aws-load-balancer-webhook -p '{"metadata":{"finalizers":[]}}' --type=merge || true
        kubectl patch mutatingwebhookconfiguration aws-load-balancer-webhook -p '{"metadata":{"finalizers":[]}}' --type=merge || true
        kubectl delete validatingwebhookconfiguration aws-load-balancer-webhook --ignore-not-found=true
        kubectl delete mutatingwebhookconfiguration aws-load-balancer-webhook --ignore-not-found=true
        
        # Clean up all AWS LB controller resources
        kubectl delete deployment aws-load-balancer-controller -n kube-system --ignore-not-found=true
        kubectl delete service aws-load-balancer-webhook-service -n kube-system --ignore-not-found=true
        kubectl delete namespace aws-load-balancer-controller --ignore-not-found=true
        
        sleep 60

    - name: Terraform Apply
      working-directory: terraform/eks/minimal
      run: |
        terraform apply -auto-approve -parallelism=1

    - name: Output cluster info
      working-directory: terraform/eks/minimal
      run: |
        echo "EKS Cluster deployed successfully!"
        terraform output configure_kubectl