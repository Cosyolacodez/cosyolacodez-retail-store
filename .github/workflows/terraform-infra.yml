
name: Terraform Infrastructure

on:
  push:
    branches: [ main ]
    paths: [ 'terraform/' ]
  pull_request:
    branches: [ main ]
    paths: [ 'terraform/' ]
  workflow_dispatch:

env:
  AWS_REGION: us-east-1
  TF_VERSION: 1.5.0
  S3_BUCKET: retail-store-terraform-state

jobs:
  terraform-plan:
    name: Terraform Plan
    runs-on: ubuntu-latest
    if: github.event_name == 'pull_request'

    steps:
    - name: Checkout
      uses: actions/checkout@v4

    - name: Setup Terraform
      uses: hashicorp/setup-terraform@v3
      with:
        terraform_version: ${{ env.TF_VERSION }}

    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ env.AWS_REGION }}

    - name: Terraform Init
      working-directory: terraform/eks/minimal
      run: |
        terraform init -backend-config="bucket=${{ env.S3_BUCKET }}" \
                        -backend-config="key=terraform.tfstate" \
                        -backend-config="region=${{ env.AWS_REGION }}" \
                        -backend-config="dynamodb_table=retail-store-terraform-locks" \
                        -backend-config="encrypt=true"

    - name: Terraform Plan
      working-directory: terraform/eks/minimal
      run: terraform plan -no-color
      continue-on-error: true

  terraform-apply:
    name: Terraform Apply
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main' && (github.event_name == 'push' || github.event_name == 'workflow_dispatch')

    steps:
    - name: Checkout
      uses: actions/checkout@v4

    - name: Setup Terraform
      uses: hashicorp/setup-terraform@v3
      with:
        terraform_version: ${{ env.TF_VERSION }}

    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ env.AWS_REGION }}

    - name: Create S3 bucket
      run: |
        aws s3api create-bucket --bucket ${{ env.S3_BUCKET }} --region ${{ env.AWS_REGION }} || echo "Bucket already exists"
        aws s3api put-bucket-versioning --bucket ${{ env.S3_BUCKET }} --versioning-configuration Status=Enabled

    - name: Terraform Init
      working-directory: terraform/eks/minimal
      run: |
        terraform init -backend-config="bucket=${{ env.S3_BUCKET }}" \
                        -backend-config="key=terraform.tfstate" \
                        -backend-config="region=${{ env.AWS_REGION }}" \
                        -backend-config="dynamodb_table=retail-store-terraform-locks" \
                        -backend-config="encrypt=true"

    - name: Clean up existing resources
      run: |
        echo "Cleaning up existing resources..."
        
        # Delete unused VPCs to avoid limit
        aws ec2 describe-vpcs --query 'Vpcs[?IsDefault==`false`].[VpcId,Tags[?Key==`Name`].Value|[0]]' --output text | while read vpc_id name; do
          if [ "$name" != "retail-store-vpc" ]; then
            echo "Deleting VPC: $vpc_id ($name)"
            aws ec2 delete-vpc --vpc-id $vpc_id 2>/dev/null || echo "Failed to delete VPC $vpc_id"
          fi
        done
        
        # Delete existing KMS alias if it exists
        aws kms delete-alias --alias-name alias/eks/retail-store 2>/dev/null || echo "KMS alias not found or already deleted"
        
        # Delete existing log group
        aws logs delete-log-group --log-group-name /aws/eks/retail-store/cluster 2>/dev/null || echo "Log group not found or already deleted"
        
        echo "Cleanup completed"

    - name: Handle Existing Resources
      working-directory: terraform/eks/minimal
      run: |
        echo "Checking terraform state..."
        terraform state list || echo "No existing state"
        echo "State check completed"

    - name: Clean up AWS Load Balancer Controller
      run: |
        aws eks update-kubeconfig --region ${{ env.AWS_REGION }} --name retail-store || echo "Cluster not found"
        
        # Delete all webhook configurations
        kubectl get validatingwebhookconfiguration -o name | grep -E '(aws-load-balancer|elbv2)' | xargs -r kubectl delete --ignore-not-found=true
        kubectl get mutatingwebhookconfiguration -o name | grep -E '(aws-load-balancer|elbv2)' | xargs -r kubectl delete --ignore-not-found=true
        
        # Force delete specific webhooks
        kubectl patch validatingwebhookconfiguration aws-load-balancer-webhook -p '{"metadata":{"finalizers":[]}}' --type=merge || true
        kubectl patch mutatingwebhookconfiguration aws-load-balancer-webhook -p '{"metadata":{"finalizers":[]}}' --type=merge || true
        kubectl delete validatingwebhookconfiguration aws-load-balancer-webhook --ignore-not-found=true
        kubectl delete mutatingwebhookconfiguration aws-load-balancer-webhook --ignore-not-found=true
        
        # Clean up all AWS LB controller resources
        kubectl delete deployment aws-load-balancer-controller -n kube-system --ignore-not-found=true
        kubectl delete service aws-load-balancer-webhook-service -n kube-system --ignore-not-found=true
        kubectl delete namespace aws-load-balancer-controller --ignore-not-found=true
        
        sleep 60

    - name: Terraform Apply
      working-directory: terraform/eks/minimal
      run: |
        terraform apply -auto-approve -parallelism=1

    - name: Output cluster info
      working-directory: terraform/eks/minimal
      run: |
        echo "EKS Cluster deployed successfully!"
        terraform output configure_kubectl